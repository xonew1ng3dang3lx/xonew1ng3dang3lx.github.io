{"componentChunkName":"component---src-templates-blog-page-markdown-index-js","path":"/blog/client-side-rendered-crawler-e08c0320","result":{"data":{"site":{"siteMetadata":{"siteUrl":"https://techwaifu.com"}},"blog":{"html":"<p>Back in the day, websites were primarily rendered server side. This made it easy for bots to crawl and extract content from the page without using Javascript. However in the last couple of years, we've seen an increase of new Javascript frameworks. With the rise of client side rendered apps, it has become a bit more difficult to crawl websites since some sites require Javascript to render. Some client side rendered pages fetch more content via infinitely scrolling making it impossible for a traditional bot to crawl.  Thankfully, we can create a bot that can render Javascript to crawl these types of web apps. </p>\n<p>In this tutorial, we'll be using <a href=\"https://github.com/puppeteer/puppeteer\">puppeteer</a> to crawl Instagram. You can use this library to control Chromium to crawl websites. </p>\n<p>Start by creating a new project with <code>npm init</code>. Add puppeteer and request to your <code>package.json</code> via:</p>\n<pre><code>npm add puppeteer request\n</code></pre>\n<p>Your <code>package.json</code> will look something like this afterwards.</p>\n<pre><code>{\n  \"name\": \"infinite-crawler\",\n  \"version\": \"1.0.0\",\n  \"description\": \"\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" &#x26;&#x26; exit 1\"\n  },\n  \"author\": \"\",\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"puppeteer\": \"^5.2.1\",\n    \"request\": \"^2.88.2\"\n  }\n}\n</code></pre>\n<p>Now let's create an <code>index.js</code> file and populate it with the following code. We'll be setting headless to false to visualize what's going on.</p>\n<pre><code>const fs = require('fs');\nconst puppeteer = require('puppeteer');\n\nconst instagramUrl = \"https://www.instagram.com/\";\n\n(async function main() {\n  // Set up browser and page.\n  const browser = await puppeteer.launch({\n    headless: false,\n    args: ['--no-sandbox', '--disable-setuid-sandbox'],\n  });\n  const page = await browser.newPage();\n  page.setViewport({ width: 1600, height: 1600 });\n\n  const url = \"https://www.instagram.com/thejenkinscomic\";\n  await page.goto(url);\n\n  // Close the browser.\n  await browser.close();\n})();\n</code></pre>\n<p>Let's test this out by running <code>node index</code>. If done correctly, you'll see Chrome open up the instagram profile for thejenkinscomic.</p>\n<p><img src=\"/blogImages/crawler/crawler.jpg?width=701&#x26;height=1005&#x26;isResponsive=true&#x26;supportsWebp=true\" alt=\"Instagram\"></p>\n<p>Next, let's add some code that will scroll through the page to fetch more Instagram posts. The <code>scrollToEndOfPage</code> function will continue scrolling until it hits the end of the page.</p>\n<p>Instagram does something interesting to optimize for performance. Instagram optimizes the number of divs on the page by reusing the previously created divs instead of creating new divs on the page. This is the same concept used by Android Apps with a <a href=\"https://developer.android.com/guide/topics/ui/layout/recyclerview\">RecyclerView</a>. Because the page is constantly changing, we'll need to scrape the page each time we scroll to make sure we have all the items. This also prevents crawlers that don't use Javascript from scraping the profile completely.</p>\n<pre><code>async function scrollToEndOfPage(\n    page,\n    extractImgLinks = () => {},\n) {\n    let items = [];\n    try {\n        let previousHeight;\n        while (true) {\n            const curHeight = await page.evaluate('document.body.scrollHeight');\n            if (previousHeight === curHeight) {\n                break;\n            }\n            previousHeight = curHeight;\n            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)');\n            await page.waitForFunction(`document.body.scrollHeight > ${previousHeight}`);\n            await page.waitFor(3500);\n            const links = await page.evaluate(extractImgLinks).catch(err => {\n                console.log(err);\n                return [];\n            });\n            items = [...items, ...links];\n        }\n    } catch (e) {\n    }\n    return items;\n}\n</code></pre>\n<p>Update the <code>main</code> function to call the scroller and then run <code>node index</code>. This time the bot will open Instagram and scroll until there are no more posts left to fetch.</p>\n<pre><code>(async function main() {\n\n  ...\n  await scrollToEndOfPage(page);\n\n  // Close the browser.\n  await browser.close();\n});\n</code></pre>\n<p><img src=\"/blogImages/crawler/srcset.jpg?width=522&#x26;height=401&#x26;isResponsive=true&#x26;supportsWebp=true\" alt=\"Instagram srcset\"></p>\n<p>Now we'll need to create a function that will scrape all the img tags. If you inspect the DOM of Instagram, you'll notice the posts have a <code>src</code> and <code>srcset</code>. For the sake of simplicity, we'll extract the <code>src</code>, but if you're feeling ambitious you can scrape the srcset and grab the highest resolution image. You can learn more about <a href=\"https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images\">srcset</a> from this link.</p>\n<pre><code>// Finds all links with images as a child.\nfunction extractImgLinks() {\n    const extractedElements = document.querySelectorAll('a img');\n    const items = [];\n    for (let element of extractedElements) {\n        items.push(element.src)\n    }\n    return items;\n}\n</code></pre>\n<p>Update the <code>scrollToEndOfPage</code> inside <code>main</code> to scrape all the images using the <code>extractImgLinks</code> function we wrote. Pass the function we just wrote to <code>scrollToEndOfPage</code>.</p>\n<pre><code>(async function main() {\n    ...\n    const links = await scrollToEndOfPage(page, extractImgLinks);\n    console.log(links);\n     \n    ...\n    await browser.close();\n});\n</code></pre>\n<p>Finally, we'll need a function to download and save all the images that we scraped.</p>\n<pre><code>function download(url, path) {\n    const fileName = url.substring(url.lastIndexOf(\"/\"));\n    const outFile = fileName.indexOf(\"?\") >= 0 ? path + fileName.substring(0, fileName.indexOf(\"?\"))  : path+ fileName;\n\n    return new Promise((resolve) => {\n        request.head(url, (err) => {\n            request(url)\n                .on('error', function (err) {\n                    console.log(err);\n                    resolve();\n                })\n                .pipe(fs.createWriteStream(outFile))\n                .on('close', () => {\n                    resolve();\n                })\n        });\n    });\n}\n</code></pre>\n<p>We'll also need to add code to create an output directory. Because Instagram reuses the DOM to render the page, we might end up with duplicate links. The <code>removeDuplicates</code> function will filter any duplicate links.</p>\n<pre><code>const fs = require('fs');\n\n(async function main() {\n    ...\n\n    // Creates a directory to store the images\n    const username = url.substring(instagramUrl.length);\n    const dir = \"./\" + username;\n    if (!fs.existsSync(dir)) {\n        fs.mkdirSync(dir);\n    }\n    \n    // Filter duplicate links and downloads all the images\n    function removeDuplicates(array) {\n        return array.filter((a, b) => array.indexOf(a) === b)\n    };\n    const linksWithoutDuplicates = removeDuplicates(links);\n    await Promise.all(links.map(item => download(item, dir)));\n});\n</code></pre>\n<p>Here's the final code. This is obviously very basic and can be updated to grab a higher resolution image via the <code>srcset</code>. Other improvements include passing the profile url by args instead of hardcoding it.</p>\n<pre><code>const fs = require('fs');\nconst puppeteer = require('puppeteer');\nconst request = require('request');\n\nconst instagramUrl = \"https://www.instagram.com/\";\n\nfunction download(url, path) {\n    const fileName = url.substring(url.lastIndexOf(\"/\"));\n    const outFile = fileName.indexOf(\"?\") >= 0 ? path + fileName.substring(0, fileName.indexOf(\"?\"))  : path+ fileName;\n\n    return new Promise((resolve) => {\n        request.head(url, (err) => {\n            request(url)\n                .on('error', function (err) {\n                    console.log(err);\n                    resolve();\n                })\n                .pipe(fs.createWriteStream(outFile))\n                .on('close', () => {\n                    resolve();\n                })\n        });\n    });\n}\n\n// Finds all links with images as a child.\nfunction extractImgLinks() {\n    const extractedElements = document.querySelectorAll('a img');\n    const items = [];\n    for (let element of extractedElements) {\n        items.push(element.src)\n    }\n    return items;\n}\n\nasync function scrollToEndOfPage(\n    page,\n    extractImgLinks = () => {},\n) {\n    let items = [];\n    try {\n        let previousHeight;\n        while (true) {\n            const curHeight = await page.evaluate('document.body.scrollHeight');\n            if (previousHeight === curHeight) {\n                break;\n            }\n            previousHeight = curHeight;\n            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)');\n            await page.waitForFunction(`document.body.scrollHeight > ${previousHeight}`);\n            await page.waitFor(3500);\n            const links = await page.evaluate(extractImgLinks).catch(err => {\n                console.log(err);\n                return [];\n            });\n            items = [...items, ...links];\n        }\n    } catch (e) {\n    }\n    return items;\n}\n\n(async function main() {\n    // Set up browser and page.\n    const browser = await puppeteer.launch({\n        headless: false,\n        args: ['--no-sandbox', '--disable-setuid-sandbox'],\n    });\n    const page = await browser.newPage();\n    page.setViewport({ width: 1600, height: 1600 });\n\n    const url = \"https://www.instagram.com/thejenkinscomic\"\n    await page.goto(url);\n\n    // Scroll and extract items from the page.\n    const links = await scrollToEndOfPage(page, extractImgLinks);\n    // Close the browser.\n    await browser.close();\n\n    // Creates a directory to store the images\n    const username = url.substring(instagramUrl.length);\n    const dir = \"./\" + username;\n    if (!fs.existsSync(dir)) {\n        fs.mkdirSync(dir);\n    }\n\n    // Downloads all the images\n    function removeDuplicates(array) {\n        return array.filter((a, b) => array.indexOf(a) === b)\n    };\n    const linksWithoutDuplicates = removeDuplicates(links);\n    await Promise.all(linksWithoutDuplicates.map(item => download(item, dir)));\n})();\n</code></pre>\n<h2>Conclusion</h2>\n<p>Now you know how to build your very own front end rendered crawler. You can adapt this to your own use-cases. What do you plan on building? Let me know in the comments section below and check out <a href=\"https://techwaifu.com/difference-between-studying-computer-science-working-software-engineer-df3bb680\">Programming in School vs Working as a Software Engineer</a>.</p>","htmlAst":{"type":"root","children":[{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Back in the day, websites were primarily rendered server side. This made it easy for bots to crawl and extract content from the page without using Javascript. However in the last couple of years, we've seen an increase of new Javascript frameworks. With the rise of client side rendered apps, it has become a bit more difficult to crawl websites since some sites require Javascript to render. Some client side rendered pages fetch more content via infinitely scrolling making it impossible for a traditional bot to crawl.  Thankfully, we can create a bot that can render Javascript to crawl these types of web apps. "}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"In this tutorial, we'll be using "},{"type":"element","tagName":"a","properties":{"href":"https://github.com/puppeteer/puppeteer"},"children":[{"type":"text","value":"puppeteer"}]},{"type":"text","value":" to crawl Instagram. You can use this library to control Chromium to crawl websites. "}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Start by creating a new project with "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"npm init"}]},{"type":"text","value":". Add puppeteer and request to your "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"package.json"}]},{"type":"text","value":" via:"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"pre","properties":{},"children":[{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"npm add puppeteer request\n"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Your "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"package.json"}]},{"type":"text","value":" will look something like this afterwards."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"pre","properties":{},"children":[{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"{\n  \"name\": \"infinite-crawler\",\n  \"version\": \"1.0.0\",\n  \"description\": \"\",\n  \"main\": \"index.js\",\n  \"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\"\n  },\n  \"author\": \"\",\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"puppeteer\": \"^5.2.1\",\n    \"request\": \"^2.88.2\"\n  }\n}\n"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Now let's create an "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"index.js"}]},{"type":"text","value":" file and populate it with the following code. We'll be setting headless to false to visualize what's going on."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"pre","properties":{},"children":[{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"const fs = require('fs');\nconst puppeteer = require('puppeteer');\n\nconst instagramUrl = \"https://www.instagram.com/\";\n\n(async function main() {\n  // Set up browser and page.\n  const browser = await puppeteer.launch({\n    headless: false,\n    args: ['--no-sandbox', '--disable-setuid-sandbox'],\n  });\n  const page = await browser.newPage();\n  page.setViewport({ width: 1600, height: 1600 });\n\n  const url = \"https://www.instagram.com/thejenkinscomic\";\n  await page.goto(url);\n\n  // Close the browser.\n  await browser.close();\n})();\n"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Let's test this out by running "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"node index"}]},{"type":"text","value":". If done correctly, you'll see Chrome open up the instagram profile for thejenkinscomic."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"/blogImages/crawler/crawler.jpg?width=701&height=1005&isResponsive=true&supportsWebp=true","alt":"Instagram"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Next, let's add some code that will scroll through the page to fetch more Instagram posts. The "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"scrollToEndOfPage"}]},{"type":"text","value":" function will continue scrolling until it hits the end of the page."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Instagram does something interesting to optimize for performance. Instagram optimizes the number of divs on the page by reusing the previously created divs instead of creating new divs on the page. This is the same concept used by Android Apps with a "},{"type":"element","tagName":"a","properties":{"href":"https://developer.android.com/guide/topics/ui/layout/recyclerview"},"children":[{"type":"text","value":"RecyclerView"}]},{"type":"text","value":". Because the page is constantly changing, we'll need to scrape the page each time we scroll to make sure we have all the items. This also prevents crawlers that don't use Javascript from scraping the profile completely."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"pre","properties":{},"children":[{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"async function scrollToEndOfPage(\n    page,\n    extractImgLinks = () => {},\n) {\n    let items = [];\n    try {\n        let previousHeight;\n        while (true) {\n            const curHeight = await page.evaluate('document.body.scrollHeight');\n            if (previousHeight === curHeight) {\n                break;\n            }\n            previousHeight = curHeight;\n            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)');\n            await page.waitForFunction(`document.body.scrollHeight > ${previousHeight}`);\n            await page.waitFor(3500);\n            const links = await page.evaluate(extractImgLinks).catch(err => {\n                console.log(err);\n                return [];\n            });\n            items = [...items, ...links];\n        }\n    } catch (e) {\n    }\n    return items;\n}\n"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Update the "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"main"}]},{"type":"text","value":" function to call the scroller and then run "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"node index"}]},{"type":"text","value":". This time the bot will open Instagram and scroll until there are no more posts left to fetch."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"pre","properties":{},"children":[{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"(async function main() {\n\n  ...\n  await scrollToEndOfPage(page);\n\n  // Close the browser.\n  await browser.close();\n});\n"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"element","tagName":"img","properties":{"src":"/blogImages/crawler/srcset.jpg?width=522&height=401&isResponsive=true&supportsWebp=true","alt":"Instagram srcset"},"children":[]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Now we'll need to create a function that will scrape all the img tags. If you inspect the DOM of Instagram, you'll notice the posts have a "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"src"}]},{"type":"text","value":" and "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"srcset"}]},{"type":"text","value":". For the sake of simplicity, we'll extract the "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"src"}]},{"type":"text","value":", but if you're feeling ambitious you can scrape the srcset and grab the highest resolution image. You can learn more about "},{"type":"element","tagName":"a","properties":{"href":"https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images"},"children":[{"type":"text","value":"srcset"}]},{"type":"text","value":" from this link."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"pre","properties":{},"children":[{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"// Finds all links with images as a child.\nfunction extractImgLinks() {\n    const extractedElements = document.querySelectorAll('a img');\n    const items = [];\n    for (let element of extractedElements) {\n        items.push(element.src)\n    }\n    return items;\n}\n"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Update the "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"scrollToEndOfPage"}]},{"type":"text","value":" inside "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"main"}]},{"type":"text","value":" to scrape all the images using the "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"extractImgLinks"}]},{"type":"text","value":" function we wrote. Pass the function we just wrote to "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"scrollToEndOfPage"}]},{"type":"text","value":"."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"pre","properties":{},"children":[{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"(async function main() {\n    ...\n    const links = await scrollToEndOfPage(page, extractImgLinks);\n    console.log(links);\n     \n    ...\n    await browser.close();\n});\n"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Finally, we'll need a function to download and save all the images that we scraped."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"pre","properties":{},"children":[{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"function download(url, path) {\n    const fileName = url.substring(url.lastIndexOf(\"/\"));\n    const outFile = fileName.indexOf(\"?\") >= 0 ? path + fileName.substring(0, fileName.indexOf(\"?\"))  : path+ fileName;\n\n    return new Promise((resolve) => {\n        request.head(url, (err) => {\n            request(url)\n                .on('error', function (err) {\n                    console.log(err);\n                    resolve();\n                })\n                .pipe(fs.createWriteStream(outFile))\n                .on('close', () => {\n                    resolve();\n                })\n        });\n    });\n}\n"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"We'll also need to add code to create an output directory. Because Instagram reuses the DOM to render the page, we might end up with duplicate links. The "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"removeDuplicates"}]},{"type":"text","value":" function will filter any duplicate links."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"pre","properties":{},"children":[{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"const fs = require('fs');\n\n(async function main() {\n    ...\n\n    // Creates a directory to store the images\n    const username = url.substring(instagramUrl.length);\n    const dir = \"./\" + username;\n    if (!fs.existsSync(dir)) {\n        fs.mkdirSync(dir);\n    }\n    \n    // Filter duplicate links and downloads all the images\n    function removeDuplicates(array) {\n        return array.filter((a, b) => array.indexOf(a) === b)\n    };\n    const linksWithoutDuplicates = removeDuplicates(links);\n    await Promise.all(links.map(item => download(item, dir)));\n});\n"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Here's the final code. This is obviously very basic and can be updated to grab a higher resolution image via the "},{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"srcset"}]},{"type":"text","value":". Other improvements include passing the profile url by args instead of hardcoding it."}]},{"type":"text","value":"\n"},{"type":"element","tagName":"pre","properties":{},"children":[{"type":"element","tagName":"code","properties":{},"children":[{"type":"text","value":"const fs = require('fs');\nconst puppeteer = require('puppeteer');\nconst request = require('request');\n\nconst instagramUrl = \"https://www.instagram.com/\";\n\nfunction download(url, path) {\n    const fileName = url.substring(url.lastIndexOf(\"/\"));\n    const outFile = fileName.indexOf(\"?\") >= 0 ? path + fileName.substring(0, fileName.indexOf(\"?\"))  : path+ fileName;\n\n    return new Promise((resolve) => {\n        request.head(url, (err) => {\n            request(url)\n                .on('error', function (err) {\n                    console.log(err);\n                    resolve();\n                })\n                .pipe(fs.createWriteStream(outFile))\n                .on('close', () => {\n                    resolve();\n                })\n        });\n    });\n}\n\n// Finds all links with images as a child.\nfunction extractImgLinks() {\n    const extractedElements = document.querySelectorAll('a img');\n    const items = [];\n    for (let element of extractedElements) {\n        items.push(element.src)\n    }\n    return items;\n}\n\nasync function scrollToEndOfPage(\n    page,\n    extractImgLinks = () => {},\n) {\n    let items = [];\n    try {\n        let previousHeight;\n        while (true) {\n            const curHeight = await page.evaluate('document.body.scrollHeight');\n            if (previousHeight === curHeight) {\n                break;\n            }\n            previousHeight = curHeight;\n            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)');\n            await page.waitForFunction(`document.body.scrollHeight > ${previousHeight}`);\n            await page.waitFor(3500);\n            const links = await page.evaluate(extractImgLinks).catch(err => {\n                console.log(err);\n                return [];\n            });\n            items = [...items, ...links];\n        }\n    } catch (e) {\n    }\n    return items;\n}\n\n(async function main() {\n    // Set up browser and page.\n    const browser = await puppeteer.launch({\n        headless: false,\n        args: ['--no-sandbox', '--disable-setuid-sandbox'],\n    });\n    const page = await browser.newPage();\n    page.setViewport({ width: 1600, height: 1600 });\n\n    const url = \"https://www.instagram.com/thejenkinscomic\"\n    await page.goto(url);\n\n    // Scroll and extract items from the page.\n    const links = await scrollToEndOfPage(page, extractImgLinks);\n    // Close the browser.\n    await browser.close();\n\n    // Creates a directory to store the images\n    const username = url.substring(instagramUrl.length);\n    const dir = \"./\" + username;\n    if (!fs.existsSync(dir)) {\n        fs.mkdirSync(dir);\n    }\n\n    // Downloads all the images\n    function removeDuplicates(array) {\n        return array.filter((a, b) => array.indexOf(a) === b)\n    };\n    const linksWithoutDuplicates = removeDuplicates(links);\n    await Promise.all(linksWithoutDuplicates.map(item => download(item, dir)));\n})();\n"}]}]},{"type":"text","value":"\n"},{"type":"element","tagName":"h2","properties":{},"children":[{"type":"text","value":"Conclusion"}]},{"type":"text","value":"\n"},{"type":"element","tagName":"p","properties":{},"children":[{"type":"text","value":"Now you know how to build your very own front end rendered crawler. You can adapt this to your own use-cases. What do you plan on building? Let me know in the comments section below and check out "},{"type":"element","tagName":"a","properties":{"href":"https://techwaifu.com/difference-between-studying-computer-science-working-software-engineer-df3bb680"},"children":[{"type":"text","value":"Programming in School vs Working as a Software Engineer"}]},{"type":"text","value":"."}]}],"data":{"quirksMode":false}},"id":"75b6c1c3-e6f6-5c1a-acd6-80d711b14de9","excerpt":"Back in the day, websites were primarily rendered server side. This made it easy for bots to crawl and extract content from the page without…","frontmatter":{"author":"awang","date":"August 15, 2020","modified_date":null,"slug":"blog/client-side-rendered-crawler-e08c0320","title":"Building a Crawler for a Client-side Rendered Web App","tags":["programming","coding","website","crawler","html","computer science"],"cover_photo":"/blogImages/crawler/cover.jpg?width=692&height=496&isResponsive=true&supportsWebp=true","related_tag":"coding","is_travel_blog":null,"has_affiliate_links":null,"is_review":null,"brand_name":null,"item_name":null,"review_description":null,"score":null},"wordCount":{"words":540},"fields":{"readingTime":{"text":"6 min read"}}},"relatedPosts":{"edges":[{"node":{"frontmatter":{"slug":"how-make-your-website-accessible-e77547a0","title":"How to make your website accessible","cover_photo":"/blogImages/accessibility/cover.jpg?width=740&height=453"}}},{"node":{"frontmatter":{"slug":"ultimate-guide-coding-on-chromebook-bb405513","title":"Setting Up Java, Python, Node, React, and Angular on a Chromebook","cover_photo":"/blogImages/chromebook_coding/cover-2.jpg?width=1742&height=1306&isResponsive=true&supportsWebp=true"}}},{"node":{"frontmatter":{"slug":"coding-on-an-ipad-190ccc96","title":"Coding on an iPad","cover_photo":"/blogImages/code_on_ipad/cover.jpg?width=1024&height=715"}}},{"node":{"frontmatter":{"slug":"difference-between-studying-computer-science-working-software-engineer-df3bb680","title":"Programming in School vs Working as a Software Engineer","cover_photo":"/blogImages/school_vs_work/cover.jpg?width=1676&height=1257&isResponsive=true&supportsWebp=true"}}},{"node":{"frontmatter":{"slug":"how-to-host-a-website-for-free-5165ea28","title":"How to host a website for free","cover_photo":"/blogImages/free_website/cover.jpg?width=874&height=667"}}},{"node":{"frontmatter":{"slug":"android-development-on-chromebook-2020-5734ed1a","title":"How to Develop Android Apps on a Chromebook in 2020","cover_photo":"/blogImages/mobile_dev_on_chromebook/cover.jpg?width=2400&height=1600&isResponsive=true&supportsWebp=true"}}},{"node":{"frontmatter":{"slug":"recommended-chromebooks-for-programming-2020-5734ed1a","title":"The Best Chromebooks for Programming in 2020","cover_photo":"/blogImages/recommended_chromebooks/acer_chromebook_spin.jpg?width=960&height=631&isResponsive=true&supportsWebp=true"}}},{"node":{"frontmatter":{"slug":"why-use-remote-procedure-calls-RPC-over-JSON-fbb4ae56","title":"Why Remote Procedure Calls (RPC) Are Better than REST","cover_photo":"/blogImages/grpc/cover.jpg?width=400&height=400"}}},{"node":{"frontmatter":{"slug":"what-is-structured-data-and-why-do-i-need-it-a3f06898","title":"What is Structured Data and Why do I need it?","cover_photo":"/blogImages/structured_data/cover.png?width=981&height=327"}}},{"node":{"frontmatter":{"slug":"importance-of-code-quality-impact-of-tech-debt-1fb19358","title":"The Importance of Code Quality and the Impact of Technical Debt","cover_photo":"/blogImages/tech_debt/cover.jpg?width=1450&height=1087&isResponsive=true&supportsWebp=true"}}}]}},"pageContext":{"slug":"blog/client-side-rendered-crawler-e08c0320","relatedTag":"coding"}}}